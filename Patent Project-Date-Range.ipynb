{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title & Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pyarrow\n",
    "import networkx as nx\n",
    "from networkx.algorithms import approximation as approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bq_helper\n",
    "from bq_helper import BigQueryHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_APPLICATION_CREDENTIALS = r\"C:\\Users\\Isaac\\GA-Python\\patents-isaac-ga-test-0795587125f7.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from BigQuery API\n",
    "We need two separate tables for this analysis: patents and citations. Citations are retrieved via the Google BigQuery API using the BigQueryHelper package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration to retrieve citation data from BQ\n",
    "bq_citations = bq_helper.BigQueryHelper(active_project=\"patents-isaac-ga-test\", dataset_name=\"uspatentcitation\")\n",
    "def get_data(year):\n",
    "    query = f\"SELECT patent_id, citation_id, date FROM patents-isaac-ga-test.uspatentdata.uspatentcitation WHERE LEFT(date,4) = '{year}';\"\n",
    "    df = bq_citations.query_to_pandas(query)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:461: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Retrieves data from BQ. Very long runtime, so should only be run once.\n",
    "#Only configured to pull one year at at time to conserve memory. \n",
    "citations_raw = get_data(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes a copy of the citation data and removes design patents (non-numeric patent_ids) \n",
    "citations = citations_raw.copy()\n",
    "mask_digit = ((citations['patent_id'].str.isdigit()) & (citations['citation_id'].str.isdigit()))\n",
    "citations = citations[mask_digit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload and clean raw patent class data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell actually retrieves the data from BigQuery. It takes between 5 and 10 minutes to run. \n",
    "bq_classes = bq_helper.BigQueryHelper(active_project=\"patents-public-data\", dataset_name=\"patentsview\")\n",
    "query1 = f\"SELECT patent_id, mainclass_id FROM patents-public-data.patentsview.uspc WHERE sequence = '0';\"\n",
    "patent_class_raw = bq_classes.query_to_pandas(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a copy of the raw BQ data, drops all design patents and design citations as denoted by non-numeric patent_ids\n",
    "patents_raw = patent_class_raw.copy()\n",
    "patents_raw['mainclass_id'] = patents_raw['mainclass_id'].astype(str)\n",
    "mask_digit2 = ((patents_raw['patent_id'].str.isdigit()) & (patents_raw['mainclass_id'].str.isdigit()))\n",
    "patents_raw = patents_raw[mask_digit2]\n",
    "patents_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Class Data into Citations File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast as string so we can use an inner join and drop everything that isn't in the list of utility patents. \n",
    "citations['patent_id'] = citations['patent_id'].astype(str)\n",
    "citations['citation_id'] = citations['citation_id'].astype(str)\n",
    "\n",
    "#merge in patent classes for citing patent\n",
    "citations = pd.merge(citations, patents_raw, how= 'inner', left_on = 'patent_id', right_on= 'patent_id')\n",
    "citations.rename(columns = {'mainclass_id':'patent_class'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge in patent classes for citations\n",
    "citations = pd.merge(citations, patents_raw, how= 'inner', left_on = 'citation_id', right_on= 'patent_id')\n",
    "citations.rename(columns = {'mainclass_id':'citation_class', 'patent_id_x':'patent_id'}, inplace = True)\n",
    "citations.drop(columns = ['patent_id_y'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Node Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get patent counts for use in the matrix later - size of bubble chart and calculation of edge weights\n",
    "class_counts = pd.DataFrame(patents_raw.groupby(['mainclass_id'])['patent_id'].count())\n",
    "class_counts.rename(columns = {'patent_id':'class_patent_count'}, inplace = True)\n",
    "class_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Map/Matrix\n",
    "The core measure we're interested in is the share of citations for a given patent and patent class that come from another class. As this likely reflects the transfer of tacit knowledge for use in inventions, we can use this as a measure of how closely related the two classes are in a given time period. \n",
    "\n",
    "This approach does assume that all patent citations are equally important, which is manifestly false, but the impact of that assumption is ultimately very small. Subsequent analysis could use a patent's subsequent citations to retroactively determine how impactful it was. We could then adjust the patent citation share accordingly. However, that analysis requires a dramatically more powerful computer than I can currently afford, and the eventual matrix is not likely to be all that different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_share = (1 / pd.DataFrame(citations.groupby(['patent_id'])['citation_id'].count()))\n",
    "citation_share.rename(columns = {'citation_id':'citation_share'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = pd.merge(citations, citation_share, how = 'left', left_on = 'patent_id', right_on = 'patent_id' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we aggregate citationshare at the class level. \n",
    "#This measure tells us roughly how many patents in a class were derived from another class. \n",
    "matrix = pd.DataFrame(citations.groupby(['patent_class', 'citation_class'])['citation_share'].sum().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge class counts into the matrix. \n",
    "matrix = pd.merge(matrix, class_counts, how = 'inner', left_on = 'patent_class', right_on= 'mainclass_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing citation_share by the total class count tells us what fraction of the patents in a class derived from another class\n",
    "matrix['edge_weight'] = (matrix['citation_share'] / matrix['class_patent_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Visualizations & Statistics\n",
    "\n",
    "With networks of this size, a normal network diagram is all but unreadable, though when comparing charts, it's easier to view compare the overall shape of the knowledge space across multiple years. \n",
    "\n",
    "More importantly, the network diagram shown below is a byproduct of preparing the data for analysis. We've only selected one measure of network cohesion -- node closeness centrality, though the networkx package has an abundance of options. \n",
    "\n",
    "Please note that because all nodes are in some way connected to nearly all other nodes in this chart, most standard matrix measures are less useful that we might prefer, which is why we're not looking at degree centrality or node connectivity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip leading zeros\n",
    "matrix['patent_class'] = matrix['patent_class'].str.lstrip(\"0\")\n",
    "matrix['citation_class'] = matrix['citation_class'].str.lstrip(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge_weight is already normalized between 0 and 1. we want larger values to be closer to zero or \"closer\" in the network\n",
    "matrix['edge_weight']=  1 - matrix['edge_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove edges that have minimal value. \n",
    "matrix2 = matrix[matrix['edge_weight'] < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create separate nodes and edges tables from matrix for easier graphing\n",
    "nodes = matrix2[['patent_class', 'class_patent_count']].drop_duplicates()\n",
    "edges = matrix2[['patent_class', 'citation_class', 'edge_weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(edges.to_records(index=False))\n",
    "nodes = list(nodes.to_records(index= False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 30))\n",
    "g = nx.Graph()\n",
    "\n",
    "for node, size in nodes:\n",
    "    g.add_node(node, size = size)\n",
    "\n",
    "for start, end, length in edges:\n",
    "    g.add_edge(start, end, length = length)\n",
    "\n",
    "nx.draw_networkx(g, arrows = True, with_labels = False, alpha = .5, node_size = 10)\n",
    "plt.savefig('2000Patents.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "class_closeness_centrality = nx.algorithms.centrality.closeness_centrality(g, distance= length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "close_nodes = pd.DataFrame(list(class_closeness_centrality.items()))\n",
    "close_nodes.rename(columns= {0:\"class\", 1:'closeness'}, inplace= True)\n",
    "close_nodes.sort_values('closeness', ascending = False, inplace= True)\n",
    "close_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "Writes the total patent citation matrix and the node closeness centrality scores to JSON files for use elsewhere. \n",
    "Note that the graph visualization was exported previously to .png. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_json('C:\\Users\\Isaac\\GA-Python\\2000matrix.json')\n",
    "close_nodes.to_json('C:\\Users\\Isaac\\GA-Python\\2000closeness.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "All data drawn from the PatentView database, which is supplied and updated by the USPTO and hosted by Google in the BigQuery Public Data project. \n",
    "\n",
    "Methods drawn from Rigby, Kogler & Tucker (2013) - Mapping Technological Relatedness in US Cities\n",
    "\n",
    "The Networkx graphing and analysis package is fantastic, powerful, and very poorly documented. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
